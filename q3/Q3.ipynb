{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = x/x.sum()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def one_hot_encode(x):\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(x)\n",
    "    return lb.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = ['vision_hackathon_drive/vision_hackathon_drive/Amy_Adams',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/bradley_cooper',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/Cobie_Smulders',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/gal_gadot',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/hrithik_roshan',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/hugh_jackman',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/Isla_Fisher',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/Katrina_Kaif',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/roger_federer',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/Saif_Ali_Khan',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/scarlett_johansson',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/shahid_kapoor',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/shraddha_kapoor',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/tom_hardy',\n",
    "'vision_hackathon_drive/vision_hackathon_drive/vani_kapoor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def get_frame(image_file):\n",
    "    image = cv2.imread(image_file,cv2.IMREAD_COLOR)\n",
    "    #print(image.shape)\n",
    "    image = cv2.resize(image, (255,255))\n",
    "    feature = np.array(image, dtype=np.float32)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "for data_dir in datasets:\n",
    "    paths = glob(\"{}/*.jpg\".format(data_dir))\n",
    "    label = data_dir.split('/')[-1]\n",
    "    #print(label)\n",
    "    for path in paths:\n",
    "        features.append(get_frame(path))\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.array([normalize(feature) for feature in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = one_hot_encode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimensions : (1664, 255, 255, 3)\n",
      "Labels dimensions : (1664, 15)\n"
     ]
    }
   ],
   "source": [
    "print('Feature dimensions : {}'.format(features.shape))\n",
    "print('Labels dimensions : {}'.format(labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    features,\n",
    "    labels,\n",
    "    test_size=0.05,\n",
    "    random_state=832289)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, nh,nw,nc, ny):\n",
    "        with tf.variable_scope('inputs'):\n",
    "            self.X = tf.placeholder(tf.float32, [None,nh,nw, nc], name='X')\n",
    "            self.Y = tf.placeholder(tf.float32,[None,ny],name='Y')\n",
    "    \n",
    "    def initialize(self):\n",
    "        W1 = tf.Variable(tf.truncated_normal([4,4,3,8], stddev=0.1), name=\"W1\")\n",
    "        W2 = tf.Variable(tf.truncated_normal([2,2,8,16], stddev=0.1), name=\"W2\")\n",
    "        print('Shape of X : ', self.X.get_shape().as_list())\n",
    "        Z1 = tf.nn.conv2d(self.X, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        print('Shape of Z1 : ', Z1.get_shape().as_list())\n",
    "        A1 = tf.nn.relu(Z1)\n",
    "        print('Shape of A1 : ', A1.get_shape().as_list())\n",
    "        P1 = tf.nn.max_pool(A1, ksize = [1, 8, 8, 1], strides = [1, 8, 8, 1], padding='SAME')\n",
    "        print('Shape of P1 : ', P1.get_shape().as_list())\n",
    "        Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        print('Shape of Z2 : ', Z2.get_shape().as_list())\n",
    "        A2 = tf.nn.relu(Z2)\n",
    "        print('Shape of A2 : ', A2.get_shape().as_list())\n",
    "        P2 = tf.nn.max_pool(A2, ksize = [1, 4, 4, 1], strides = [1, 4, 4, 1], padding='SAME')\n",
    "        print('Shape of P2 : ', P2.get_shape().as_list())\n",
    "        P = tf.keras.layers.flatten(P2)\n",
    "        print('Shape of P : ', P.get_shape().as_list())\n",
    "        self.Z3 = tf.contrib.layers.fully_connected(P, 15, activation_fn=None)\n",
    "        print('Shape of Z3 : ', self.Z3.get_shape().as_list())\n",
    "                \n",
    "    def add_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.Z3, labels=self.Y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "    def add_optimizer(self,global_step,learning_rate=1.e-3,adam_beta1=0.9,adam_beta2=0.999):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, adam_beta1, adam_beta2)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n",
    "            self.gradients = gradients\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "                self.optimize = optimizer.apply_gradients(zip(clipped_gradients, variables),global_step=global_step)\n",
    "                \n",
    "    def add_accuracy(self):\n",
    "        # Determine if the predictions are correct\n",
    "        is_correct_prediction = tf.equal(tf.argmax(self.Z3, 1), tf.argmax(self.Y, 1))\n",
    "        # Calculate the accuracy of the predictions\n",
    "        accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(255,255,3,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Shape of X :  [None, 255, 255, 3]\n",
      "Shape of Z1 :  [None, 255, 255, 8]\n",
      "Shape of A1 :  [None, 255, 255, 8]\n",
      "Shape of P1 :  [None, 32, 32, 8]\n",
      "Shape of Z2 :  [None, 32, 32, 16]\n",
      "Shape of A2 :  [None, 32, 32, 16]\n",
      "Shape of P2 :  [None, 8, 8, 16]\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "Shape of P :  [None, 1024]\n",
      "Shape of Z3 :  [None, 15]\n",
      "WARNING:tensorflow:From <ipython-input-17-420028ec0c45>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "model.initialize()\n",
    "model.add_loss()\n",
    "model.add_optimizer(global_step)\n",
    "model.add_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : {100} , Loss = {2.704194}\n",
      "Step : {200} , Loss = {2.664703}\n",
      "Step : {300} , Loss = {2.688957}\n",
      "Step : {400} , Loss = {2.694161}\n",
      "Step : {500} , Loss = {2.612667}\n",
      "Step : {600} , Loss = {2.678887}\n",
      "Step : {700} , Loss = {2.592733}\n",
      "Step : {800} , Loss = {2.558969}\n",
      "Step : {900} , Loss = {2.552173}\n",
      "Step : {1000} , Loss = {2.667575}\n",
      "Step : {1100} , Loss = {2.548392}\n",
      "Step : {1200} , Loss = {2.583253}\n",
      "Step : {1300} , Loss = {2.549084}\n",
      "Step : {1400} , Loss = {2.528803}\n",
      "Step : {1500} , Loss = {2.675372}\n",
      "Step : {1600} , Loss = {2.577952}\n",
      "Step : {1700} , Loss = {2.359047}\n",
      "Step : {1800} , Loss = {2.520584}\n",
      "Step : {1900} , Loss = {2.480792}\n",
      "Step : {2000} , Loss = {2.486034}\n",
      "Step : {2100} , Loss = {2.804366}\n",
      "Step : {2200} , Loss = {2.311068}\n",
      "Step : {2300} , Loss = {2.598475}\n",
      "Step : {2400} , Loss = {2.592263}\n",
      "Step : {2500} , Loss = {2.320297}\n",
      "Step : {2600} , Loss = {2.134440}\n",
      "Step : {2700} , Loss = {1.987363}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 100\n",
    "\n",
    "for epoch in range(num_of_epochs):\n",
    "    batches = batch_iter(list(zip(train_features,train_labels)),32,100)\n",
    "    for batch in batches:\n",
    "        x_batch,y_batch = zip(*batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        step,loss,_ = sess.run([global_step,model.loss,model.optimize],feed_dict = {model.X:x_batch, model.Y:y_batch})\n",
    "        if (step % 100 == 0):\n",
    "            print('Step : {%d} , Loss = {%f}' % (step,loss))\n",
    "    if epoch % 10 == 0:\n",
    "        training_accuracy = sess.run(self.accuracy,feed_dict={model.X:train_features,model.Y:train_labels})\n",
    "        validation_accuracy = sess.run(self.accuracy,feed_dict={model.X:valid_labels, model.Y:valid_labels})\n",
    "        print('Training accuracy at {}'.format(training_accuracy))\n",
    "        print('Validation accuracy at {}'.format(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
